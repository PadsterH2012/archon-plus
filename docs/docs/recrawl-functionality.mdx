---
title: Recrawl Functionality
description: Complete guide to knowledge base recrawl features and implementation
---

# Recrawl Functionality

The recrawl functionality allows users to refresh and update existing knowledge base sources, ensuring content stays current and accurate. This guide covers the complete recrawl system implementation and usage.

## Overview

Recrawling enables:
- **Content Updates:** Refresh outdated documentation and code
- **Error Recovery:** Retry failed crawls with improved processing
- **Configuration Changes:** Apply new extraction settings to existing content
- **Quality Improvement:** Re-process content with enhanced algorithms

## User Interface

### Recrawl Button

Every knowledge item card includes a recrawl button that triggers content refresh:

```typescript
// KnowledgeItemCard.tsx
<button
  onClick={handleRefresh}
  className="flex items-center gap-1 mb-1 px-2 py-1 transition-colors"
  title={
    item.metadata.source_type === 'url'
      ? `Refresh from: ${item.metadata.original_url}`
      : `Recrawl uploaded file: ${item.source_id}`
  }
>
  <RefreshCw className="w-3 h-3" />
  <span className="text-xs">Recrawl</span>
</button>
```

### Progress Tracking

Real-time progress updates are displayed during recrawl operations:

- **WebSocket Integration:** Live progress updates
- **Status Indicators:** Visual feedback on crawl stages
- **Error Reporting:** Clear error messages and recovery options
- **Cancellation Support:** Ability to stop running crawls

## API Implementation

### Refresh Endpoint

**Endpoint:** `POST /api/knowledge-items/{source_id}/refresh`

**Purpose:** Triggers recrawl of an existing knowledge source

**Implementation:**
```python
@router.post("/knowledge-items/{source_id}/refresh")
async def refresh_knowledge_item(source_id: str):
    """Refresh a knowledge item by re-crawling its URL with the same metadata."""
    
    # Get existing knowledge item
    service = KnowledgeItemService(get_supabase_client())
    existing_item = await service.get_item(source_id)
    
    if not existing_item:
        raise HTTPException(status_code=404, detail="Knowledge item not found")
    
    # Extract metadata for recrawl
    metadata = existing_item.get("metadata", {})
    url = existing_item.get("url")
    
    # Handle file sources vs URL sources
    if not url and source_id.startswith("file_"):
        # Reconstruct content from database chunks
        url = f"raw:{reconstructed_content}"
    
    # Create crawl request
    request_dict = {
        "url": url,
        "knowledge_type": metadata.get("knowledge_type", "technical"),
        "tags": metadata.get("tags", []),
        "max_depth": metadata.get("max_depth", 2),
        "extract_code_examples": True,
        "generate_summary": True,
    }
    
    # Start background crawl task
    progress_id = str(uuid.uuid4())
    await crawl_service.orchestrate_crawl(request_dict)
    
    return {"success": True, "progress_id": progress_id}
```

### Progress Management

**WebSocket Integration:**
```python
# Real-time progress updates
await update_crawl_progress(progress_id, {
    "status": "crawling",
    "percentage": 45,
    "log": "Processing content...",
    "currentUrl": url,
    "processedPages": 3,
    "totalPages": 7
})
```

**Progress States:**
- `starting` - Initializing crawl process
- `analyzing` - Detecting URL type and content structure
- `crawling` - Fetching content from source
- `processing` - Chunking and embedding content
- `code_extraction` - Extracting and processing code examples
- `completed` - Successfully finished
- `error` - Failed with error details
- `cancelled` - User-cancelled operation

## Content Processing Pipeline

### 1. Source Validation

Before recrawling, the system validates the source:

```python
# Verify source exists and is accessible
existing_item = await service.get_item(source_id)
if not existing_item:
    raise HTTPException(status_code=404, detail="Source not found")

# Check if source has valid URL or reconstructable content
url = existing_item.get("url")
if not url and source_id.startswith("file_"):
    # Handle file sources by reconstructing from database
    url = await reconstruct_file_content(source_id)
```

### 2. Content Cleanup

Before processing new content, existing data is cleaned:

```python
# Remove existing chunks
supabase_client.table("archon_crawled_pages").delete().eq("source_id", source_id).execute()

# Remove existing code examples
supabase_client.table("archon_code_examples").delete().eq("source_id", source_id).execute()

# Preserve source metadata
# (source record in archon_sources is updated, not deleted)
```

### 3. Fresh Processing

The recrawl uses the same processing pipeline as initial crawls:

- **Content Fetching:** Re-download or reconstruct source content
- **Chunking:** Apply current chunking strategy and settings
- **Embedding Generation:** Create new embeddings with current model
- **Code Extraction:** Apply latest extraction rules and filters
- **Storage:** Store updated content with new timestamps

## File Source Handling

### Content Reconstruction

For uploaded files, recrawl reconstructs the original content:

```python
async def reconstruct_file_content(source_id: str) -> str:
    """Reconstruct original file content from database chunks."""
    
    # Get all content chunks for this source
    pages_response = (
        supabase_client.from_("archon_crawled_pages")
        .select("content, url")
        .eq("source_id", source_id)
        .order("id")
        .execute()
    )
    
    if not pages_response.data:
        raise HTTPException(status_code=400, detail="No content found to refresh")
    
    # Combine chunks back into original content
    combined_content = "\n\n".join([page["content"] for page in pages_response.data])
    
    return f"raw:{combined_content}"
```

### Metadata Preservation

File metadata is preserved during recrawl:

```python
# Preserve original file information
file_metadata = {
    "original_filename": metadata.get("original_filename"),
    "file_size": metadata.get("file_size"),
    "upload_timestamp": metadata.get("upload_timestamp"),
    "file_type": metadata.get("file_type")
}
```

## URL Source Handling

### URL Validation

For web sources, the system validates URL accessibility:

```python
# Check if URL is still accessible
try:
    response = await http_client.head(url, timeout=10)
    if response.status_code >= 400:
        raise HTTPException(status_code=400, detail=f"URL not accessible: {response.status_code}")
except Exception as e:
    raise HTTPException(status_code=400, detail=f"URL validation failed: {str(e)}")
```

### Content Comparison

The system can optionally compare new content with existing:

```python
# Optional: Check if content has actually changed
if metadata.get("check_content_changes", False):
    new_content_hash = hashlib.md5(new_content.encode()).hexdigest()
    old_content_hash = existing_item.get("content_hash")
    
    if new_content_hash == old_content_hash:
        return {"success": True, "message": "Content unchanged, skipping recrawl"}
```

## Error Handling

### Common Error Scenarios

1. **Source Not Found:** Original source no longer exists
2. **Access Denied:** Permissions changed since original crawl
3. **Content Too Large:** Source grew beyond processing limits
4. **Network Issues:** Temporary connectivity problems
5. **Processing Failures:** Errors in chunking or embedding generation

### Recovery Strategies

```python
# Graceful error handling with fallback options
try:
    await crawl_service.orchestrate_crawl(request_dict)
except SourceNotFoundError:
    # Offer to mark source as inactive
    await mark_source_inactive(source_id, "Source no longer accessible")
except ProcessingError as e:
    # Retry with reduced settings
    request_dict["max_depth"] = 1
    request_dict["extract_code_examples"] = False
    await crawl_service.orchestrate_crawl(request_dict)
except Exception as e:
    # Log error and preserve existing content
    logger.error(f"Recrawl failed for {source_id}: {e}")
    await update_crawl_progress(progress_id, {
        "status": "error",
        "error": str(e),
        "message": "Recrawl failed, existing content preserved"
    })
```

## Configuration Options

### Recrawl Settings

Users can configure recrawl behavior:

```typescript
interface RecrawlOptions {
  preserveExistingOnFailure: boolean;    // Keep old content if recrawl fails
  checkContentChanges: boolean;          // Skip if content unchanged
  updateMetadata: boolean;               // Refresh source metadata
  reprocessCodeExamples: boolean;        // Re-extract code with new settings
  maxRetries: number;                    // Retry attempts for failures
  timeoutSeconds: number;                // Maximum time for recrawl
}
```

### Batch Recrawl

For multiple sources:

```python
@router.post("/knowledge-items/batch-refresh")
async def batch_refresh_knowledge_items(source_ids: list[str]):
    """Refresh multiple knowledge items in parallel."""
    
    results = []
    semaphore = asyncio.Semaphore(3)  # Limit concurrent recrawls
    
    async def refresh_single(source_id: str):
        async with semaphore:
            try:
                result = await refresh_knowledge_item(source_id)
                return {"source_id": source_id, "success": True, "result": result}
            except Exception as e:
                return {"source_id": source_id, "success": False, "error": str(e)}
    
    tasks = [refresh_single(sid) for sid in source_ids]
    results = await asyncio.gather(*tasks)
    
    return {"results": results}
```

## Monitoring and Analytics

### Recrawl Metrics

Track recrawl performance and success rates:

```python
# Metrics to monitor
recrawl_metrics = {
    "total_recrawls": 0,
    "successful_recrawls": 0,
    "failed_recrawls": 0,
    "average_duration": 0,
    "content_change_rate": 0,  # Percentage of recrawls with actual changes
    "error_types": {},         # Categorized error frequencies
}
```

### Performance Optimization

- **Caching:** Cache unchanged content to avoid reprocessing
- **Incremental Updates:** Only update changed sections when possible
- **Priority Queuing:** Prioritize user-requested recrawls
- **Resource Management:** Limit concurrent recrawls to prevent overload

## Best Practices

### When to Recrawl

- **Regular Schedule:** Set up automatic recrawls for frequently updated sources
- **Content Changes:** Trigger recrawls when source content is known to have changed
- **Error Recovery:** Use recrawl to fix failed or incomplete initial crawls
- **Settings Updates:** Recrawl after changing extraction or processing settings

### Optimization Tips

1. **Batch Operations:** Group related recrawls to improve efficiency
2. **Off-Peak Scheduling:** Run large recrawl operations during low-usage periods
3. **Selective Recrawling:** Only recrawl sources that actually need updates
4. **Progress Monitoring:** Use WebSocket updates to provide user feedback

## Related Documentation

- [Knowledge Base Troubleshooting](./knowledge-base-troubleshooting.mdx)
- [Code Extraction Architecture](./code-extraction-architecture.mdx)
- [Crawling Configuration](./crawling-configuration.mdx)
- [API Reference](./api-reference.mdx)
